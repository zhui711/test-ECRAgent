"""
Phase 3 Memory-Augmented Judge (Unified Version)
=================================================

ç»Ÿä¸€ç‰ˆ Judge Agentï¼Œä½œä¸º Phase 3 çš„å”¯ä¸€çœŸæºã€‚
æ ¹æ® PHASE3_REFACTOR_PLAN.md é‡æ„ï¼Œæ¶ˆé™¤æ—§ç‰ˆ phase3_debate çš„æ•°æ®æ–­è£‚ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä» State è¯»å– graph_summary å’Œ naive_scoresï¼ˆç”± Phase 2 ç”Ÿæˆï¼‰
2. ä½¿ç”¨ Phase 2 Graph ä¸­çš„ P-Nodes è¿›è¡Œ Memory Bank æ£€ç´¢
3. å°†æ£€ç´¢ç»“æœä½œä¸º Few-Shot Context æ³¨å…¥ Prompt
4. åŸºäº Evidence è¿›è¡Œå…¬å¹³ç«äº‰ï¼ˆç§»é™¤ Risk Level Safeguardï¼‰

è®¾è®¡å†³ç­–ï¼š
- æ£€ç´¢æº: Phase 2 Graph P-Nodesï¼ˆæ›´å‡†ç¡®çš„ç—‡çŠ¶é›†åˆï¼‰
- Risk Level: å½»åº•ç§»é™¤ï¼Œæ‰€æœ‰å€™é€‰ä»…å‡­ Evidence & Naive Score ç«äº‰
- Shadow æŒ‡ä»¤: ä¸ä»¥ Shadow æ•°é‡æƒ©ç½šï¼Œä»…æ£€æŸ¥ Pathognomonic ç¼ºå¤±
- Naive Score: ä½œä¸ºå‚è€ƒåŸºçº¿ï¼Œå…è®¸åŒ»å­¦æ¨ç†è¦†ç›–
"""

import json
from typing import Dict, Any, Optional, List

from .phase3_debate import JudgeAgent
from src.utils.api_client import LLMClient
from src.memory.memory_bank import MemoryBankManager
from src.utils.prompt_utils import DIAGNOSIS_ID_MAP
from src.utils.json_utils import parse_json_from_text
from config.prompt_phase3_judge import PHASE3_JUDGE_SYSTEM_PROMPT


class MemoryAugmentedJudge(JudgeAgent):
    """
    Memory-Augmented Judge Agent
    
    ç»§æ‰¿ JudgeAgentï¼Œå¢åŠ  Memory Bank æ£€ç´¢å’Œ Few-Shot æ³¨å…¥åŠŸèƒ½ã€‚
    
    å·¥ä½œæµç¨‹ï¼š
    1. ä» Memory Bank æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹ (2 Overturn + 2 Confirm)
    2. æ ¼å¼åŒ–ä¸º Few-Shot Context (éšè—æ ‡ç­¾)
    3. å°† Context æ³¨å…¥åˆ° Prompt ä¸­
    4. æ‰§è¡ŒåŸæœ‰çš„ Tiered Logic åˆ¤å†³
    
    Attributes:
        memory_bank: Memory Bank ç®¡ç†å™¨
    """
    
    def __init__(
        self,
        llm_client: LLMClient,
        model_name: str = "qwen3-32b",
        memory_bank: Optional[MemoryBankManager] = None,
        memory_bank_dir: str = "memory_bank"
    ):
        """
        åˆå§‹åŒ– Memory-Augmented Judge
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            model_name: æ¨¡å‹åç§°
            memory_bank: å·²åŠ è½½çš„ Memory Bank å®ä¾‹ (å¯é€‰)
            memory_bank_dir: Memory Bank ç›®å½• (å¦‚æœæœªæä¾› memory_bank)
        """
        super().__init__(llm_client, model_name)
        
        # åˆå§‹åŒ–æˆ–ä½¿ç”¨ä¼ å…¥çš„ Memory Bank
        if memory_bank is not None:
            self.memory_bank = memory_bank
        else:
            self.memory_bank = MemoryBankManager(output_dir=memory_bank_dir)
            try:
                self.memory_bank.load()
                print(f"[MemoryAugmentedJudge] Loaded Memory Bank: "
                      f"{self.memory_bank.get_statistics()}")
            except Exception as e:
                print(f"[MemoryAugmentedJudge] Warning: Failed to load Memory Bank: {e}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "total_retrievals": 0,
            "successful_retrievals": 0,
            "avg_similarity_overturn": 0.0,
            "avg_similarity_confirm": 0.0
        }
    
    def process(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†çŠ¶æ€ï¼Œæ‰§è¡Œ Memory-Augmented Phase 3 åˆ¤å†³ï¼ˆç»Ÿä¸€ç‰ˆï¼‰
        
        é‡æ„åçš„ä¸»æµç¨‹ï¼š
        1. ä» State è¯»å– graph_summary å’Œ naive_scores
        2. JIT Retrieval: ä½¿ç”¨ Phase 2 Graph P-Nodes æ£€ç´¢ Memory Bank
        3. æ„å»º Prompt å¹¶è°ƒç”¨ LLM
        4. å½»åº•ç§»é™¤ _pre_audit è°ƒç”¨ï¼ˆRisk Level ä¸å†å‚ä¸ï¼‰
        
        Args:
            state: LangGraph çŠ¶æ€å­—å…¸
        
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸ï¼ˆåŒ…å« final_output, memory_recordsï¼‰
        """
        try:
            # ========== 1. è¯»å– State ==========
            graph_json = state.get("graph_json")
            if not graph_json:
                return {
                    **state,
                    "status": "failed",
                    "error_log": "Graph JSON is missing"
                }
            
            phase1_result = state.get("phase1_result", {})
            input_case = state.get("input_case", {})
            raw_text = input_case.get("narrative", "")
            
            # ä» State è¯»å– graph_summary å’Œ naive_scoresï¼ˆç”± Phase 2 Summarizer ç”Ÿæˆï¼‰
            graph_summary = state.get("graph_summary", "")
            naive_scores = state.get("naive_scores", {})
            
            # ========== 2. JIT Memory Retrieval ==========
            # æ£€æŸ¥ state.memory_recordsï¼Œå¦‚æœä¸ºç©ºåˆ™æ‰§è¡Œæ£€ç´¢
            memory_records = state.get("memory_records") or []
            
            if not memory_records:
                # ä½¿ç”¨ Phase 2 Graph çš„ P-Nodes è¿›è¡Œæ£€ç´¢ï¼ˆæ›´å‡†ç¡®ï¼‰
                memory_records = self._retrieve_from_graph_p_nodes(graph_json)
                # æ›´æ–° State
                state["memory_records"] = memory_records
            
            # æ ¼å¼åŒ– Few-Shot Context
            few_shot_context = self._format_few_shot_from_records(memory_records)
            
            # ========== 3. æ„å»º Prompt ==========
            # æ³¨æ„ï¼šå½»åº•ç§»é™¤ _pre_audit è°ƒç”¨ï¼Œæ‰€æœ‰å€™é€‰ä»…å‡­ Evidence ç«äº‰
            user_prompt = self._construct_unified_prompt(
                graph_json=graph_json,
                phase1_result=phase1_result,
                raw_text=raw_text,
                graph_summary=graph_summary,
                naive_scores=naive_scores,
                few_shot_context=few_shot_context
            )
            
            # ========== 4. LLM è°ƒç”¨ ==========
            messages = [
                {"role": "system", "content": PHASE3_JUDGE_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ]
            
            result = self.llm_client.generate_json(
                messages=messages,
                model=self.model_name,
                logprobs=False,
                temperature=0.2,
                max_tokens=4096
            )
            
            if result["error"]:
                print(f"[MemoryAugmentedJudge] LLM error: {result['error']}")
                return {
                    **state,
                    "status": "failed",
                    "error_log": f"LLM error: {result['error']}"
                }
            
            # ========== 5. è§£æè¾“å‡º ==========
            response_content = result["content"]
            parsed_json = parse_json_from_text(response_content, verbose=True)
            
            if not parsed_json:
                print(f"[MemoryAugmentedJudge] Failed to parse JSON")
                return {
                    **state,
                    "status": "failed",
                    "error_log": "Failed to parse JSON from LLM response"
                }
            
            # éªŒè¯å¹¶ä¿®æ­£è¯Šæ–­ IDï¼ˆä½¿ç”¨ç®€åŒ–ç‰ˆï¼Œä¸ä¾èµ– audit_resultsï¼‰
            final_output = self._validate_output_unified(parsed_json, phase1_result)
            
            # æ·»åŠ  Memory Retrieval ä¿¡æ¯
            final_output["memory_retrieval_used"] = bool(memory_records)
            
            print(f"[MemoryAugmentedJudge] Final diagnosis: {final_output.get('final_diagnosis_id')} - "
                  f"{final_output.get('final_diagnosis_name')} ({final_output.get('status')})")
            
            return {
                **state,
                "final_output": final_output,
                "memory_records": memory_records,
                "status": "success"
            }
            
        except Exception as e:
            import traceback
            error_msg = f"Phase 3 Memory-Augmented error: {str(e)}\n{traceback.format_exc()}"
            print(f"[MemoryAugmentedJudge] {error_msg}")
            return {
                **state,
                "status": "failed",
                "error_log": error_msg
            }
    
    def _retrieve_from_graph_p_nodes(
        self,
        graph_json: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Phase 2 Graph çš„ P-Nodes è¿›è¡Œ Memory Bank æ£€ç´¢
        
        è¿™æ˜¯ JIT (Just-In-Time) æ£€ç´¢ç­–ç•¥çš„æ ¸å¿ƒå®ç°ã€‚
        ä½¿ç”¨ Phase 2 äº§å‡ºçš„ P-Nodesï¼ˆç»è¿‡éªŒè¯å’Œæ•´ç†çš„ç—‡çŠ¶é›†åˆï¼‰ã€‚
        
        Args:
            graph_json: Phase 2 è¾“å‡ºçš„å›¾è°± JSON
        
        Returns:
            æ£€ç´¢åˆ°çš„å®Œæ•´ Payload åˆ—è¡¨
        """
        self._stats["total_retrievals"] += 1
        
        try:
            if self.memory_bank is None:
                print("[MemoryAugmentedJudge] Memory Bank not initialized")
                return []
            
            # ä» Phase 2 Graph æå– P-Nodes
            p_nodes = graph_json.get("graph", {}).get("nodes", {}).get("p_nodes", [])
            
            if not p_nodes:
                print("[MemoryAugmentedJudge] No P-Nodes in graph for retrieval")
                return []
            
            # æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹ (2 Overturn + 2 Confirm)
            similar_cases = self.memory_bank.retrieve_similar(
                query_p_nodes=p_nodes,
                n_overturn=2,
                n_confirm=2
            )
            
            overturn_cases = similar_cases.get("overturn", [])
            confirm_cases = similar_cases.get("confirm", [])
            
            all_cases = overturn_cases + confirm_cases
            
            if not all_cases:
                print("[MemoryAugmentedJudge] No similar cases found")
                return []
            
            self._stats["successful_retrievals"] += 1
            
            # æ›´æ–°ç›¸ä¼¼åº¦ç»Ÿè®¡
            if overturn_cases:
                avg_sim = sum(c.get("similarity_score", 0) for c in overturn_cases) / len(overturn_cases)
                self._stats["avg_similarity_overturn"] = avg_sim
            if confirm_cases:
                avg_sim = sum(c.get("similarity_score", 0) for c in confirm_cases) / len(confirm_cases)
                self._stats["avg_similarity_confirm"] = avg_sim
            
            print(f"[MemoryAugmentedJudge] Retrieved {len(overturn_cases)} Overturn + {len(confirm_cases)} Confirm cases")
            return all_cases
            
        except Exception as e:
            print(f"[MemoryAugmentedJudge] Retrieval error: {e}")
            return []
    
    def _format_few_shot_from_records(
        self,
        memory_records: List[Dict[str, Any]]
    ) -> str:
        """
        ä» Memory Bank å®Œæ•´ Payload æ ¼å¼åŒ– Few-Shot Context
        
        å…³é”®å­—æ®µæå–ï¼š
        - ground_truth_name: æ­£ç¡®ç­”æ¡ˆ
        - initial_diagnosis_name: åˆå§‹è¯Šæ–­
        - final_diagnosis_name: æœ€ç»ˆè¯Šæ–­
        - outcome: Confirm/Overturn
        - p_nodes_summary: æ‚£è€…ç—‡çŠ¶æ‘˜è¦
        - similarity_score: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå±•ç¤ºä¾› LLM å‚è€ƒï¼‰
        
        Args:
            memory_records: Memory Bank æ£€ç´¢åˆ°çš„å®Œæ•´ Payload åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„ Few-Shot Context å­—ç¬¦ä¸²
        """
        if not memory_records:
            return ""
        
        lines = ["### ğŸ§  SIMILAR HISTORICAL CASES"]
        lines.append("*Reference cases from training data. Use their reasoning patterns as guidance.*\n")
        
        for idx, case in enumerate(memory_records, 1):
            # æå–å…³é”®å­—æ®µ
            initial = case.get("initial_diagnosis_name", "Unknown")
            final = case.get("final_diagnosis_name", "Unknown")
            ground_truth = case.get("ground_truth_name", "Unknown")
            outcome = case.get("outcome", "Unknown")
            p_summary = case.get("p_nodes_summary", "N/A")
            similarity = case.get("similarity_score", 0.0)
            
            # åŠ¨æ€åˆ¤æ–­ Correctness
            is_correct = (final == ground_truth)
            correctness = "âœ… Correct" if is_correct else "âŒ Incorrect"
            
            lines.append(f"**[Case {idx}]** (Similarity: {similarity:.2f})")
            lines.append(f"  - Patient Summary: {p_summary}")
            lines.append(f"  - Initial Diagnosis: {initial}")
            lines.append(f"  - Final Diagnosis: {final} ({correctness})")
            lines.append(f"  - Outcome: {outcome}")
            
            # Key Insight
            if outcome == "Overturn":
                lines.append(f"  - ğŸ’¡ Insight: Changed from {initial} to {final}")
            else:
                lines.append(f"  - ğŸ’¡ Insight: Confirmed initial diagnosis {initial}")
            lines.append("")
        
        return "\n".join(lines)
    
    # ==================== æ—§ç‰ˆæ–¹æ³• (ä¿ç•™å…¼å®¹æ€§ï¼Œæ ‡è®°ä¸º Deprecated) ====================
    
    def _retrieve_and_format_context(
        self,
        phase1_result: Dict[str, Any]
    ) -> str:
        """
        [DEPRECATED] ä½¿ç”¨ Phase 1 P-Nodes æ£€ç´¢
        
        å·²è¢« _retrieve_from_graph_p_nodes æ›¿ä»£ã€‚
        ä¿ç•™æ­¤æ–¹æ³•ä»…ä¸ºå…¼å®¹æ—§ä»£ç ã€‚
        """
        # è°ƒç”¨æ–°æ–¹æ³•çš„é€»è¾‘
        track_b_output = phase1_result.get("track_b_output", {})
        p_nodes = track_b_output.get("p_nodes", [])
        
        if not p_nodes or self.memory_bank is None:
            return ""
        
        try:
            similar_cases = self.memory_bank.retrieve_similar(
                query_p_nodes=p_nodes,
                n_overturn=2,
                n_confirm=2
            )
            all_cases = similar_cases.get("overturn", []) + similar_cases.get("confirm", [])
            return self._format_few_shot_from_records(all_cases)
        except Exception as e:
            print(f"[MemoryAugmentedJudge] Deprecated retrieval error: {e}")
            return ""
    
    def _format_hidden_label_context(
        self,
        overturn_cases: List[Dict[str, Any]],
        confirm_cases: List[Dict[str, Any]]
    ) -> str:
        """
        [DEPRECATED] å·²è¢« _format_few_shot_from_records æ›¿ä»£
        """
        all_cases = overturn_cases + confirm_cases
        return self._format_few_shot_from_records(all_cases)
    
    def _construct_unified_prompt(
        self,
        graph_json: Dict[str, Any],
        phase1_result: Dict[str, Any],
        raw_text: str,
        graph_summary: str,
        naive_scores: Dict[str, float],
        few_shot_context: str
    ) -> str:
        """
        æ„å»ºç»Ÿä¸€ç‰ˆ Promptï¼ˆæ—  Risk Levelã€æ—  Pre-Auditï¼‰
        
        æ ¸å¿ƒå˜æ›´ï¼š
        1. ä½¿ç”¨ Rich Structured Summary æ›¿ä»£ JSON
        2. æ·»åŠ  Shadow æŒ‡ä»¤ï¼šä¸ä»¥æ•°é‡æƒ©ç½šï¼Œä»…æ£€æŸ¥ Pathognomonic ç¼ºå¤±
        3. æ·»åŠ  Naive Score è¯´æ˜ï¼šä»…ä¾›å‚è€ƒï¼Œå…è®¸åŒ»å­¦æ¨ç†è¦†ç›–
        4. å½»åº•ç§»é™¤ Pre-Audit ä¿¡æ¯
        
        Args:
            graph_json: Phase 2 å›¾è°±
            phase1_result: Phase 1 ç»“æœ
            raw_text: åŸå§‹ç—…å†
            graph_summary: Rich Structured Summary
            naive_scores: æœ´ç´ è¯„åˆ†
            few_shot_context: Few-Shot Context
        
        Returns:
            å®Œæ•´çš„ User Prompt
        """
        # æå–åˆå§‹å€™é€‰
        initial_candidates = phase1_result.get("top_candidates", [])
        initial_candidates_list = ", ".join([
            f"{DIAGNOSIS_ID_MAP.get(cid, 'Unknown')} (ID: {cid})"
            for cid in initial_candidates
        ])
        initial_reasoning = phase1_result.get("differential_reasoning", "")
        
        # æ„å»º Naive Scores Sectionï¼ˆå¸¦å…¬å¼è¯´æ˜ï¼‰
        naive_scores_section = self._format_naive_scores_unified(
            naive_scores or {},
            initial_candidates
        )
        
        # æ„å»ºå®Œæ•´ Prompt
        return f"""
## PATIENT CASE

{raw_text}

## PHASE 1 INITIAL ASSESSMENT

**Initial Candidates:** {initial_candidates_list}

**Differential Reasoning:**
{initial_reasoning}

## ğŸ“Š NAIVE SCORES (Reference Baseline)

{naive_scores_section}

## âš–ï¸ RICH EVIDENCE SUMMARY

{graph_summary}

{few_shot_context}

## YOUR TASK

Based on the above evidence, determine the **final diagnosis**.

### Decision Framework:

1. **Fatal Conflict Check (Tier 1):**
   - If a candidate has a **Conflict** on an **Essential/Pathognomonic** feature (P-Node status = "Absent"), it is **disqualified**.
   - âš ï¸ **Shadow â‰  Conflict**: Missing symptoms (Shadow) are NOT disqualifying factors.

2. **Pivot Competition (Tier 2):**
   - Candidates with **matched Pivot Features** are superior to those without.
   - If a lower-ranked candidate has Pivot support but Top-1 does not, consider **Overturn**.

3. **Evidence Coverage (Tier 3):**
   - Use **Naive Scores** as a quantitative baseline.
   - Higher score = more supporting evidence.

### âš ï¸ CRITICAL INSTRUCTIONS ON SHADOW NODES:

1. **Do NOT penalize candidates based on the *count* or *number* of Shadow nodes.**
2. **Only penalize if CRITICAL Pathognomonic evidence is missing:**
   - Example: Missing "D-dimer elevation" for Pulmonary Embolism = significant concern.
   - Example: Missing "Fatigue" = ignore (not a deal-breaker).
3. **Naive Score Context:** The score formula uses ShadowÃ—0.1 penalty. If a candidate's score appears low due to many irrelevant Shadows, use your **medical judgment to override** the score.

### Output Format (JSON):

```json
{{
    "final_diagnosis_id": "XX",
    "final_diagnosis_name": "Disease Name",
    "status": "Confirm|Overturn|Fallback",
    "reasoning_path": "Step-by-step reasoning explaining your decision...",
    "audit_log": ["Key decision points..."]
}}
```
"""
    
    def _format_naive_scores_unified(
        self,
        naive_scores: Dict[str, float],
        top_candidates: List[str]
    ) -> str:
        """
        æ ¼å¼åŒ– Naive Scoresï¼ˆç»Ÿä¸€ç‰ˆï¼Œå¸¦å…¬å¼è¯´æ˜ï¼‰
        
        Args:
            naive_scores: è¯„åˆ†å­—å…¸
            top_candidates: Phase 1 Top Candidates
        
        Returns:
            æ ¼å¼åŒ–çš„ Naive Scores å­—ç¬¦ä¸²
        """
        if not naive_scores:
            return "*No Naive Scores available*"
        
        lines = ["**Formula:** `Score = (Match Ã— 1.0) - (Conflict Ã— 1.5) - (Shadow Ã— 0.1)`"]
        lines.append("")
        lines.append("*Note: Shadow penalty (0.1) is minimal. Do NOT over-penalize missing evidence.*")
        lines.append("")
        
        # æ‰¾åˆ°æœ€é«˜åˆ†
        max_score = max(naive_scores.values()) if naive_scores else 0
        top1_score = naive_scores.get(f"d_{top_candidates[0]}", 0) if top_candidates else 0
        
        # æŒ‰ Phase 1 Rank æ’åˆ—
        for rank, cand_id in enumerate(top_candidates, 1):
            d_id = f"d_{cand_id}"
            score = naive_scores.get(d_id, 0.0)
            
            # æ ‡æ³¨ä¿¡å·
            signal = ""
            if rank > 1 and score > top1_score:
                signal = " â¬†ï¸ **Higher than Top-1 (Potential Overturn Signal)**"
            elif score == max_score and score > 0:
                signal = " ğŸ† **Highest**"
            
            disease_name = DIAGNOSIS_ID_MAP.get(cand_id, "Unknown")
            score_str = f"+{score:.1f}" if score >= 0 else f"{score:.1f}"
            lines.append(f"- **{d_id}** ({disease_name}) [Rank {rank}]: Score = **{score_str}**{signal}")
        
        return "\n".join(lines)
    
    def _validate_output_unified(
        self,
        parsed_json: Dict[str, Any],
        phase1_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        éªŒè¯å¹¶ä¿®æ­£ LLM è¾“å‡ºï¼ˆç»Ÿä¸€ç‰ˆï¼Œä¸ä¾èµ– audit_resultsï¼‰
        
        Args:
            parsed_json: LLM è¾“å‡ºçš„ JSON
            phase1_result: Phase 1 ç»“æœ
        
        Returns:
            éªŒè¯åçš„ final_output
        """
        final_diagnosis_id = parsed_json.get("final_diagnosis_id")
        
        # éªŒè¯ ID æœ‰æ•ˆæ€§
        if not final_diagnosis_id or final_diagnosis_id not in DIAGNOSIS_ID_MAP:
            # Fallback ç­–ç•¥ï¼šä½¿ç”¨ Phase 1 çš„ Top-1
            top_candidates = phase1_result.get("top_candidates", [])
            
            if top_candidates and top_candidates[0] in DIAGNOSIS_ID_MAP:
                fallback_id = top_candidates[0]
                print(f"[MemoryAugmentedJudge] Invalid ID '{final_diagnosis_id}', using fallback: {fallback_id}")
                parsed_json["final_diagnosis_id"] = fallback_id
                parsed_json["final_diagnosis_name"] = DIAGNOSIS_ID_MAP[fallback_id]
                parsed_json["status"] = "Fallback"
            elif "01" in DIAGNOSIS_ID_MAP:
                parsed_json["final_diagnosis_id"] = "01"
                parsed_json["final_diagnosis_name"] = DIAGNOSIS_ID_MAP["01"]
                parsed_json["status"] = "Emergency_Fallback"
        else:
            # ID æœ‰æ•ˆï¼Œç¡®ä¿åç§°æ­£ç¡®
            parsed_json["final_diagnosis_name"] = DIAGNOSIS_ID_MAP.get(
                final_diagnosis_id,
                parsed_json.get("final_diagnosis_name", "Unknown")
            )
            
            # ç¡®å®šçŠ¶æ€
            phase1_final_id = phase1_result.get("final_diagnosis_id", "")
            if final_diagnosis_id == phase1_final_id:
                parsed_json["status"] = "Confirm"
            else:
                parsed_json["status"] = "Overturn"
        
        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        if "reasoning_path" not in parsed_json:
            parsed_json["reasoning_path"] = "Diagnosis selected based on graph analysis."
        if "audit_log" not in parsed_json:
            parsed_json["audit_log"] = []
        
        return parsed_json
    
    # ==================== æ—§ç‰ˆ Prompt æ–¹æ³• (ä¿ç•™å…¼å®¹æ€§) ====================
    
    def _construct_memory_augmented_prompt(
        self,
        graph_json: Dict[str, Any],
        phase1_result: Dict[str, Any],
        raw_text: str,
        audit_results: Dict[str, Any],
        graph_summary: str,
        naive_scores: Dict[str, float],
        few_shot_context: str
    ) -> str:
        """
        [DEPRECATED] æ—§ç‰ˆ Prompt æ„å»ºæ–¹æ³•
        
        å·²è¢« _construct_unified_prompt æ›¿ä»£ã€‚
        ä¿ç•™æ­¤æ–¹æ³•ä»…ä¸ºå…¼å®¹æ—§ä»£ç ã€‚
        """
        return self._construct_unified_prompt(
            graph_json=graph_json,
            phase1_result=phase1_result,
            raw_text=raw_text,
            graph_summary=graph_summary,
            naive_scores=naive_scores,
            few_shot_context=few_shot_context
        )
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å– Memory Retrieval ç»Ÿè®¡ä¿¡æ¯"""
        return self._stats.copy()


# ==================== å·¥å‚å‡½æ•° ====================

def create_judge_agent(
    llm_client: LLMClient,
    model_name: str = "qwen3-32b",
    use_memory: bool = True,
    memory_bank_dir: str = "memory_bank",
    memory_bank: Optional[MemoryBankManager] = None
) -> MemoryAugmentedJudge:
    """
    åˆ›å»º Judge Agent çš„å·¥å‚å‡½æ•°ï¼ˆç»Ÿä¸€ç‰ˆï¼‰
    
    é‡æ„åï¼šå§‹ç»ˆè¿”å› MemoryAugmentedJudgeã€‚
    use_memory å‚æ•°ä»…æ§åˆ¶æ˜¯å¦æ‰§è¡Œ Memory Bank æ£€ç´¢ï¼Œ
    ä½†ç±»æœ¬èº«ä¸å˜ï¼Œç¡®ä¿æ•°æ®æµä¸€è‡´æ€§ã€‚
    
    Args:
        llm_client: LLM å®¢æˆ·ç«¯
        model_name: æ¨¡å‹åç§°
        use_memory: æ˜¯å¦ä½¿ç”¨ Memory Bank æ£€ç´¢ï¼ˆä¸å½±å“ç±»é€‰æ‹©ï¼‰
        memory_bank_dir: Memory Bank ç›®å½•
        memory_bank: å·²åŠ è½½çš„ Memory Bank å®ä¾‹
    
    Returns:
        MemoryAugmentedJudge å®ä¾‹ï¼ˆç»Ÿä¸€ï¼‰
    """
    # å§‹ç»ˆä½¿ç”¨ MemoryAugmentedJudgeï¼Œç¡®ä¿æ•°æ®æµä¸€è‡´
    if use_memory:
        return MemoryAugmentedJudge(
            llm_client=llm_client,
            model_name=model_name,
            memory_bank=memory_bank,
            memory_bank_dir=memory_bank_dir
        )
    else:
        # å³ä½¿ä¸ä½¿ç”¨ Memoryï¼Œä¹Ÿè¿”å› MemoryAugmentedJudge
        # åªæ˜¯ä¸ä¼ å…¥ memory_bankï¼Œæ£€ç´¢æ—¶ä¼šè¿”å›ç©ºç»“æœ
        return MemoryAugmentedJudge(
            llm_client=llm_client,
            model_name=model_name,
            memory_bank=None,
            memory_bank_dir=memory_bank_dir
        )


# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == "__main__":
    print("MemoryAugmentedJudge module loaded successfully")
    print("Use create_judge_agent() to instantiate")




