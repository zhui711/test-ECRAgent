"""
# ============================================================================
# DEPRECATED: æ­¤æ¨¡å—å·²è¢« phase3_memory_judge.py æ›¿ä»£
# ============================================================================
#
# è¯·ä½¿ç”¨ src.agents.phase3_memory_judge.MemoryAugmentedJudge ä½œä¸º Phase 3 çš„å”¯ä¸€å®ç°ã€‚
#
# ä¿ç•™æ­¤æ–‡ä»¶çš„åŸå› ï¼š
# 1. MemoryAugmentedJudge ç»§æ‰¿äº† JudgeAgentï¼ˆéœ€è¦åŸºç±»ï¼‰
# 2. å…¼å®¹æ—§ä»£ç çš„ import è¯­å¥
#
# ä¸å»ºè®®ç›´æ¥ä½¿ç”¨æ­¤æ¨¡å—ä¸­çš„ JudgeAgent ç±»è¿›è¡Œæ–°å¼€å‘ã€‚
# ============================================================================

Phase 3 Agent: Judge & Tiered Logic (LEGACY)
å®ç°å…ƒè®¤çŸ¥å®¡åˆ¤ï¼ŒåŸºäº Tiered Logic è¾“å‡ºæœ€ç»ˆè¯Šæ–­

æ ¹æ®ç®—æ³•æµç¨‹å›¾ Phase 3 (Line 40-56):
- Round 1: Safety Sentinel (Clinical Importance) - Fatal Conflict æ£€æŸ¥
- Round 2: Shadow Interrogation (Bias Exclusion) - ç¡®è®¤åå·®æ ¡æ­£
- Round 3: Final Verdict - æœ€ç»ˆè£å†³

æ³¨æ„: æ­¤å®ç°ä¸­çš„ _pre_audit å’Œ Risk Level é€»è¾‘å·²åœ¨æ–°ç‰ˆä¸­è¢«ç§»é™¤ã€‚
"""
import json
import re
import yaml
from pathlib import Path
from typing import Dict, Any, Optional, List
from src.utils.api_client import LLMClient
from src.utils.prompt_utils import DIAGNOSIS_ID_MAP
from src.utils.json_utils import parse_json_from_text
from config.prompt_phase3_judge import PHASE3_JUDGE_SYSTEM_PROMPT, PHASE3_JUDGE_USER_PROMPT_TEMPLATE


class JudgeAgent:
    """
    Phase 3 Judge Agent
    è´Ÿè´£æœ€ç»ˆè¯Šæ–­å†³ç­–ï¼Œåº”ç”¨ Tiered Logic
    
    æ ¸å¿ƒæµç¨‹ï¼ˆåŸºäºç®—æ³•æµç¨‹å›¾ Line 40-56ï¼‰ï¼š
    1. Round 1: Safety Sentinel - æ£€æŸ¥ High Risk ç–¾ç—…çš„ Fatal Conflict
    2. Round 2: Shadow Interrogation - è®¡ç®— Shadow Ratioï¼Œæ ¡æ­£ç¡®è®¤åå·®
    3. Round 3: Final Verdict - åŸºäº Explanatory Coverage é€‰æ‹©æœ€ç»ˆè¯Šæ–­
    """
    
    def __init__(self, llm_client: LLMClient, model_name: str = "gpt-4o"):
        """
        åˆå§‹åŒ– JudgeAgent
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯å®ä¾‹
            model_name: æ¨¡å‹åç§°
        """
        self.llm_client = llm_client
        self.model_name = model_name
        self.risk_map = self._load_risk_map()
        
        # Shadow Ratio é˜ˆå€¼ (Line 50)
        self.shadow_threshold = 0.5
    
    def _load_risk_map(self) -> Dict[str, Dict[str, str]]:
        """
        åŠ è½½ç–¾ç—…é£é™©åœ°å›¾
        
        æ ¹æ®ç®—æ³•æµç¨‹å›¾ Line 41:
        RiskMap <- LoadMetadata(D_all)  # Static High/Low Risk Tags
        
        Returns:
            å­—å…¸ï¼š{disease_id: {"name": str, "risk": str}}
        """
        try:
            config_path = Path(__file__).parent.parent.parent / "config" / "disease_metadata.yaml"
            with open(config_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
                risk_map = {}
                for line in content.split('\n'):
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    # åŒ¹é…æ ¼å¼: "01": { "name": "...", "risk": "High" }
                    match = re.match(r'"(\d+)":\s*\{\s*"name":\s*"([^"]+)",\s*"risk":\s*"([^"]+)"\s*\}', line)
                    if match:
                        disease_id, name, risk = match.groups()
                        risk_map[disease_id] = {"name": name, "risk": risk}
                
                return risk_map
        except Exception as e:
            print(f"[JudgeAgent] Error loading risk map: {e}")
            return {}
    
    def process(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†çŠ¶æ€ï¼Œæ‰§è¡Œ Phase 3 çš„æœ€ç»ˆåˆ¤å†³
        
        æ ¹æ®ç®—æ³•æµç¨‹å›¾ Line 40-56
        
        Args:
            state: LangGraph çŠ¶æ€å­—å…¸
        
        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸ï¼ˆåŒ…å« final_outputï¼‰
        """
        try:
            # è·å–è¾“å…¥
            graph_json = state.get("graph_json")
            if not graph_json:
                return {
                    **state,
                    "status": "failed",
                    "error_log": "Graph JSON is missing"
                }
            
            phase1_result = state.get("phase1_result", {})
            input_case = state.get("input_case", {})
            raw_text = input_case.get("narrative", "")
            
            # è·å– graph_summary (æ–°å¢)
            graph_summary = state.get("graph_summary", "")
            
            # è·å– naive_scores (æ–°å¢)
            naive_scores = state.get("naive_scores", {})
            
            # é¢„å¤„ç†ï¼šæ‰§è¡Œ Python å±‚é¢çš„å®¡è®¡é€»è¾‘
            audit_results = self._pre_audit(graph_json, phase1_result)
            
            # æ„å»º Prompt (ä½¿ç”¨ graph_summary å’Œ naive_scores)
            user_prompt = self._construct_judge_prompt(
                graph_json,
                phase1_result,
                raw_text,
                audit_results,
                graph_summary=graph_summary,
                naive_scores=naive_scores  # æ–°å¢å‚æ•°
            )
            
            # è°ƒç”¨ LLM
            messages = [
                {"role": "system", "content": PHASE3_JUDGE_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ]
            
            result = self.llm_client.generate_json(
                messages=messages,
                model=self.model_name,
                logprobs=False,
                temperature=0.2,  # ä½¿ç”¨ 0.2 ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º
                max_tokens=4096
            )
            
            if result["error"]:
                print(f"[JudgeAgent] LLM error: {result['error']}")
                return {
                    **state,
                    "status": "failed",
                    "error_log": f"LLM error: {result['error']}"
                }
            
            # è§£æè¾“å‡º
            response_content = result["content"]
            print(f"[JudgeAgent] LLM response length: {len(response_content)} chars")
            
            parsed_json = parse_json_from_text(response_content, verbose=True)
            
            if not parsed_json:
                print(f"[JudgeAgent] Failed to parse JSON")
                print(f"[JudgeAgent] Raw response preview: {response_content[:500]}...")
                return {
                    **state,
                    "status": "failed",
                    "error_log": "Failed to parse JSON from LLM response"
                }
            
            # éªŒè¯å¹¶ä¿®æ­£è¯Šæ–­ ID
            final_output = self._validate_and_fix_output(parsed_json, phase1_result, audit_results)
            
            print(f"[JudgeAgent] Final diagnosis: {final_output.get('final_diagnosis_id')} - "
                  f"{final_output.get('final_diagnosis_name')} ({final_output.get('status')})")
            
            return {
                **state,
                "final_output": final_output,
                "status": "success"
            }
            
        except Exception as e:
            import traceback
            error_msg = f"Phase 3 error: {str(e)}\n{traceback.format_exc()}"
            print(f"[JudgeAgent] {error_msg}")
            return {
                **state,
                "status": "failed",
                "error_log": error_msg
            }
    
    def _pre_audit(
        self, 
        graph_json: Dict[str, Any], 
        phase1_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        é¢„å®¡è®¡ï¼šåœ¨è°ƒç”¨ LLM ä¹‹å‰æ‰§è¡Œ Python å±‚é¢çš„é€»è¾‘æ£€æŸ¥
        
        æ ¹æ®ç®—æ³•æµç¨‹å›¾ Line 42-52
        """
        audit_results = {
            "disqualified_candidates": [],
            "safeguarded_candidates": [],
            "shadow_penalties": {},
            "coverage_scores": {}
        }
        
        d_nodes = graph_json.get("graph", {}).get("nodes", {}).get("d_nodes", [])
        k_nodes = graph_json.get("graph", {}).get("nodes", {}).get("k_nodes", [])
        p_nodes = graph_json.get("graph", {}).get("nodes", {}).get("p_nodes", [])
        p_k_links = graph_json.get("graph", {}).get("edges", {}).get("p_k_links", [])
        k_d_links = graph_json.get("graph", {}).get("edges", {}).get("k_d_links", [])
        
        # æ„å»ºå¿«é€ŸæŸ¥æ‰¾ç´¢å¼•
        k_node_map = {k["id"]: k for k in k_nodes}
        p_node_map = {p["id"]: p for p in p_nodes}
        
        # Round 1: Safety Sentinel (Line 42-47)
        for d_node in d_nodes:
            d_id = d_node["id"]
            original_id = d_node.get("original_id", "")
            risk_info = self.risk_map.get(original_id, {})
            risk_level = risk_info.get("risk", "Low")
            
            # æ£€æŸ¥ Fatal Conflict
            has_fatal_conflict = self._check_fatal_conflict(
                d_id, k_d_links, p_k_links, k_node_map, p_node_map
            )
            
            if has_fatal_conflict:
                audit_results["disqualified_candidates"].append({
                    "d_id": d_id,
                    "reason": "Fatal Conflict (Essential symptom absent)"
                })
            elif risk_level == "High":
                # Line 44-45: é«˜é£é™©ç–¾ç—…æ²¡æœ‰ Fatal Conflictï¼Œè®¾ç½® safeguard
                audit_results["safeguarded_candidates"].append({
                    "d_id": d_id,
                    "reason": "High Risk disease without Fatal Conflict"
                })
        
        # Round 2: Shadow Interrogation (Line 48-52)
        initial_d_id = f"d_{phase1_result.get('final_diagnosis_id', phase1_result.get('top_candidates', [''])[0])}"
        
        for d_node in d_nodes:
            d_id = d_node["id"]
            
            # è®¡ç®— Shadow Ratio (Line 49)
            shadow_count, total_count = self._count_shadows(
                d_id, k_d_links, p_k_links
            )
            
            if total_count > 0:
                shadow_ratio = shadow_count / total_count
                audit_results["shadow_penalties"][d_id] = {
                    "shadow_count": shadow_count,
                    "total_count": total_count,
                    "ratio": shadow_ratio
                }
                
                # Line 50-51: å¦‚æœæ˜¯åˆå§‹è¯Šæ–­ä¸” Shadow Ratio è¿‡é«˜ï¼Œåº”ç”¨æƒ©ç½š
                if d_id == initial_d_id and shadow_ratio > self.shadow_threshold:
                    audit_results["shadow_penalties"][d_id]["penalty_applied"] = True
            
            # Round 3: è®¡ç®— Explanatory Coverage (Line 54-55)
            coverage = self._calculate_coverage(d_id, k_d_links, p_k_links)
            audit_results["coverage_scores"][d_id] = coverage
        
        return audit_results
    
    def _check_fatal_conflict(
        self,
        d_id: str,
        k_d_links: List[Dict],
        p_k_links: List[Dict],
        k_node_map: Dict,
        p_node_map: Dict
    ) -> bool:
        """
        æ£€æŸ¥ D-Node æ˜¯å¦æœ‰ Fatal Conflict
        
        æ ¹æ®ç®—æ³•æµç¨‹å›¾ Line 22-24 å’Œ Phase 3 Tier 1:
        å¦‚æœ K.importance == "Essential" ä¸” P-K.relation == "Conflict"
        """
        # æ‰¾åˆ°ä¸è¯¥ D-Node å…³è”çš„æ‰€æœ‰ K-Nodes
        related_k_ids = [
            link["source"] for link in k_d_links 
            if link["target"] == d_id
        ]
        
        for k_id in related_k_ids:
            k_node = k_node_map.get(k_id, {})
            importance = k_node.get("importance", "Common")
            
            # åªæ£€æŸ¥ Essential æˆ– Pathognomonic
            if importance not in ["Essential", "Pathognomonic"]:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ Conflict è¾¹
            for pk_link in p_k_links:
                if pk_link["target"] == k_id and pk_link["relation"] == "Conflict":
                    return True
        
        return False
    
    def _count_shadows(
        self,
        d_id: str,
        k_d_links: List[Dict],
        p_k_links: List[Dict]
    ) -> tuple:
        """
        è®¡ç®—ä¸ D-Node å…³è”çš„ Shadow Nodes æ•°é‡
        
        Returns:
            (shadow_count, total_count)
        """
        # æ‰¾åˆ°ä¸è¯¥ D-Node å…³è”çš„æ‰€æœ‰ K-Nodes
        related_k_ids = [
            link["source"] for link in k_d_links 
            if link["target"] == d_id
        ]
        
        shadow_count = 0
        total_count = len(related_k_ids)
        
        for k_id in related_k_ids:
            # æ£€æŸ¥è¯¥ K-Node æ˜¯å¦æœ‰ Void è¾¹
            for pk_link in p_k_links:
                if pk_link["target"] == k_id and pk_link["relation"] == "Void":
                    shadow_count += 1
                    break
        
        return shadow_count, total_count
    
    def _calculate_coverage(
        self,
        d_id: str,
        k_d_links: List[Dict],
        p_k_links: List[Dict]
    ) -> float:
        """
        è®¡ç®— Explanatory Coverage
        
        æ ¹æ®ç®—æ³•æµç¨‹å›¾ Line 54-55:
        d* <- argmax_{dâˆˆD_active} (ExplanatoryCoverage(d, P))
        
        å…¬å¼: Match æ•°é‡ - Void æ•°é‡ * 0.1 - Conflict æ•°é‡ * 1.5
        """
        related_k_ids = [
            link["source"] for link in k_d_links 
            if link["target"] == d_id
        ]
        
        match_count = 0
        void_count = 0
        conflict_count = 0
        
        for k_id in related_k_ids:
            for pk_link in p_k_links:
                if pk_link["target"] == k_id:
                    relation = pk_link["relation"]
                    if relation == "Match":
                        match_count += 1
                    elif relation == "Void":
                        void_count += 1
                    elif relation == "Conflict":
                        conflict_count += 1
        
        coverage = match_count - (void_count * 0.1) - (conflict_count * 1.5)
        return coverage
    
    def _construct_judge_prompt(
        self,
        graph_json: Dict[str, Any],
        phase1_result: Dict[str, Any],
        raw_text: str,
        audit_results: Dict[str, Any],
        graph_summary: str = "",
        naive_scores: Dict[str, float] = None
    ) -> str:
        """
        æ„å»º Judge Prompt
        
        ä¿®è®¢ï¼šä½¿ç”¨ graph_summary æ›¿æ¢ graph_edges_json (æ¶ˆèå®éªŒ)
        ä¿ç•™ p_nodes_json ç”¨äºè¦†ç›–ç‡è®¡ç®—
        æ–°å¢ï¼šæ³¨å…¥ naive_scores ä½œä¸ºé‡åŒ–å‚è€ƒ
        """
        # æå– P-Nodes (ä¿ç•™)
        p_nodes = graph_json.get("graph", {}).get("nodes", {}).get("p_nodes", [])
        p_nodes_json = json.dumps(p_nodes, ensure_ascii=False, indent=2)
        
        # æå– D-Nodes ç”¨äº Low Evidence å¤„ç†
        d_nodes = graph_json.get("graph", {}).get("nodes", {}).get("d_nodes", [])
        
        # è·å– Low Evidence ç–¾ç—…åˆ—è¡¨
        low_evidence_diseases = graph_json.get("low_evidence_diseases", [])
        knowledge_sources = graph_json.get("knowledge_sources", {})
        
        # æ·»åŠ  Risk Level å’Œ Evidence Status åˆ° D-Nodes
        for d_node in d_nodes:
            original_id = d_node.get("original_id", "")
            risk_info = self.risk_map.get(original_id, {})
            d_node["risk_level"] = risk_info.get("risk", "Low")
            
            # æ·»åŠ çŸ¥è¯†æ¥æºä¿¡æ¯
            d_id = d_node.get("id", "")
            d_node["knowledge_source"] = knowledge_sources.get(d_id, "Unknown")
            
            # æ ‡è®° Low Evidence
            disease_name = d_node.get("name", "")
            if disease_name in low_evidence_diseases:
                d_node["low_evidence"] = True
            else:
                d_node["low_evidence"] = False
        
        # æ„å»º Pre-Audit ä¿¡æ¯ (ä¿ç•™å…³é”®å®¡è®¡ç»“æœ)
        pre_audit_info = {
            "disqualified_candidates": audit_results.get("disqualified_candidates", []),
            "safeguarded_candidates": audit_results.get("safeguarded_candidates", []),
                "coverage_scores": audit_results.get("coverage_scores", {})
        }
        pre_audit_json = json.dumps(pre_audit_info, ensure_ascii=False, indent=2)
        
        # æ„å»º Low Evidence è­¦å‘Š
        low_evidence_warning = ""
        if low_evidence_diseases:
            low_evidence_warning = (
                "### âš ï¸ LOW EVIDENCE WARNING\n"
                f"The following diseases have limited knowledge (search returned no results): {', '.join(low_evidence_diseases)}\n"
                "**Do NOT automatically disqualify these diseases!** Consider them based on:\n"
                "1. Existing Match edges (if any)\n"
                "2. Absence of Fatal Conflicts\n"
                "3. Clinical reasoning from the patient narrative\n"
            )
        
        # æå–åˆå§‹å€™é€‰
        initial_candidates = phase1_result.get("top_candidates", [])
        initial_candidates_list = ", ".join([
            f"{DIAGNOSIS_ID_MAP.get(cid, 'Unknown')} (ID: {cid})"
            for cid in initial_candidates
        ])
        
        initial_reasoning = phase1_result.get("differential_reasoning", "")
        
        # ä½¿ç”¨ graph_summary æ›¿æ¢ graph_edges_json (æ¶ˆèå®éªŒ)
        if graph_summary:
            # æ–°æ ¼å¼ï¼šä½¿ç”¨è‡ªç„¶è¯­è¨€æ‘˜è¦
            graph_evidence_section = f"""
### GRAPH EVIDENCE SUMMARY (Structured Natural Language)

{graph_summary}

### PRE-AUDIT RESULTS (System Computed)

```json
{pre_audit_json}
```
"""
        else:
            # é™çº§ï¼šå¦‚æœæ²¡æœ‰ summaryï¼Œä½¿ç”¨åŸå§‹ JSON (å…¼å®¹æ€§)
            k_nodes = graph_json.get("graph", {}).get("nodes", {}).get("k_nodes", [])
            edges = graph_json.get("graph", {}).get("edges", {})
            
            graph_edges_info = {
                "p_k_links": edges.get("p_k_links", []),
                "k_d_links": edges.get("k_d_links", []),
                "k_nodes": k_nodes,
                "d_nodes": d_nodes,
                "pre_audit": pre_audit_info
            }
            graph_edges_json = json.dumps(graph_edges_info, ensure_ascii=False, indent=2)
            graph_evidence_section = f"""
### GRAPH EVIDENCE (JSON Format - Fallback)

```json
{graph_edges_json}
```
"""
        
        # æ„å»º Naive Scores å±•ç¤ºï¼ˆæ–¹æ¡ˆ Bï¼šå¸¦ Phase 1 ä¸Šä¸‹æ–‡ï¼‰
        naive_scores_section = self._format_naive_scores(
            naive_scores or {},
            phase1_result.get("top_candidates", [])
        )
        
        # æ ¼å¼åŒ– Prompt (ä½¿ç”¨ä¿®æ”¹åçš„æ¨¡æ¿)
        user_prompt = self._format_judge_prompt_v2(
            raw_text=raw_text,
            initial_candidates_list=initial_candidates_list,
            initial_reasoning=initial_reasoning,
            p_nodes_json=p_nodes_json,
            graph_evidence_section=graph_evidence_section,
            low_evidence_warning=low_evidence_warning,
            naive_scores_section=naive_scores_section
        )
        
        return user_prompt
    
    def _format_naive_scores(
        self,
        naive_scores: Dict[str, float],
        top_candidates: List[str]
    ) -> str:
        """
        æ ¼å¼åŒ– Naive Scores ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²
        
        æ–¹æ¡ˆ Bï¼šæŒ‰ Phase 1 åŸå§‹ Rank æ’åˆ— + åˆ†æ•°æ ‡æ³¨
        """
        if not naive_scores:
            return ""
        
        lines = ["### ğŸ“Š NAIVE SCORES (Evidence Strength Reference)\n"]
        lines.append("*Formula: Score = (Match Ã— 1.0) - (Conflict Ã— 1.5) - (Shadow Ã— 0.1)*\n")
        
        # æ‰¾åˆ°æœ€é«˜åˆ†
        max_score = max(naive_scores.values()) if naive_scores else 0
        top1_score = naive_scores.get(f"d_{top_candidates[0]}", 0) if top_candidates else 0
        
        # æŒ‰ Phase 1 Rank æ’åˆ—
        for rank, cand_id in enumerate(top_candidates, 1):
            d_id = f"d_{cand_id}"
            score = naive_scores.get(d_id, 0.0)
            
            # æ ‡æ³¨é€†è½¬ä¿¡å·
            signal = ""
            if rank > 1 and score > top1_score:
                signal = " â¬†ï¸ **Higher than Top-1**"
            elif score == max_score and score > 0:
                signal = " ğŸ†"
            
            disease_name = DIAGNOSIS_ID_MAP.get(cand_id, "Unknown")
            lines.append(f"- **{d_id}** ({disease_name}) [Phase1 Rank: {rank}]: Score = **{score:.1f}**{signal}")
        
        lines.append("")
        return "\n".join(lines)
    
    def _format_judge_prompt_v2(
        self,
        raw_text: str,
        initial_candidates_list: str,
        initial_reasoning: str,
        p_nodes_json: str,
        graph_evidence_section: str,
        low_evidence_warning: str,
        naive_scores_section: str = ""
    ) -> str:
        """
        æ ¼å¼åŒ– Judge Prompt V2 (ä½¿ç”¨ graph_summary å’Œ naive_scores)
        
        ä¿æŒä¸åŸ PHASE3_JUDGE_USER_PROMPT_TEMPLATE çš„å…¼å®¹æ€§ï¼Œ
        ä½†å°† graph_edges_json æ›¿æ¢ä¸º graph_evidence_section
        """
        # ä½¿ç”¨å†…ç½®æ¨¡æ¿ï¼ˆå¸¦ naive_scoresï¼‰
        return f"""
## PATIENT CASE

{raw_text}

## PHASE 1 INITIAL ASSESSMENT

**Initial Candidates:** {initial_candidates_list}

**Differential Reasoning:**
{initial_reasoning}

## PATIENT FINDINGS (P-Nodes)

```json
{p_nodes_json}
```

{graph_evidence_section}

{naive_scores_section}

{low_evidence_warning}

## YOUR TASK

Based on the above evidence, determine the final diagnosis. Apply the Tiered Logic:

1. **Safety Sentinel (Tier 1):** Check for Fatal Conflicts on Essential/Pathognomonic features (P-Node status must be "Absent", not just missing)
2. **Pivot Competition (Tier 2):** Compare Pivot support - candidates with matched Pivot Features are superior
3. **Coverage Audit (Tier 3):** Use Naive Scores as tie-breaker; select highest coverage

âš ï¸ **IMPORTANT:** A "Missing" symptom (Shadow) is NOT a Conflict. Only "Absent" (patient denied) causes Fatal Conflict.

Output your decision in JSON format:
```json
{{
    "final_diagnosis_id": "XX",
    "final_diagnosis_name": "Disease Name",
    "status": "Confirm|Overturn|Fallback",
    "reasoning_path": "Step-by-step reasoning...",
    "audit_log": ["Key decision points..."]
}}
```
"""
    
    def _validate_and_fix_output(
        self,
        parsed_json: Dict[str, Any],
        phase1_result: Dict[str, Any],
        audit_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        éªŒè¯å¹¶ä¿®æ­£ LLM è¾“å‡º
        
        Phase 3 å¿…é¡»åšå‡ºé€‰æ‹©ã€‚å¦‚æœ LLM è¾“å‡ºæ— æ•ˆçš„è¯Šæ–­ IDï¼Œ
        ä½¿ç”¨ fallback ç­–ç•¥ã€‚
        """
        final_diagnosis_id = parsed_json.get("final_diagnosis_id")
        
        # éªŒè¯ ID æœ‰æ•ˆæ€§
        if not final_diagnosis_id or final_diagnosis_id not in DIAGNOSIS_ID_MAP:
            # Fallback ç­–ç•¥
            top_candidates = phase1_result.get("top_candidates", [])
            
            # ä¼˜å…ˆä½¿ç”¨ Phase 1 çš„ Top-1ï¼ˆå¦‚æœæ²¡æœ‰è¢« disqualifyï¼‰
            disqualified_ids = [
                d["d_id"].replace("d_", "") 
                for d in audit_results.get("disqualified_candidates", [])
            ]
            
            for candidate_id in top_candidates:
                if candidate_id in DIAGNOSIS_ID_MAP and candidate_id not in disqualified_ids:
                    print(f"[JudgeAgent] Invalid ID '{final_diagnosis_id}', using fallback: {candidate_id}")
                    parsed_json["final_diagnosis_id"] = candidate_id
                    parsed_json["final_diagnosis_name"] = DIAGNOSIS_ID_MAP[candidate_id]
                    parsed_json["status"] = "Fallback"
                    break
            else:
                # æœ€åçš„ fallback
                if top_candidates and top_candidates[0] in DIAGNOSIS_ID_MAP:
                    parsed_json["final_diagnosis_id"] = top_candidates[0]
                    parsed_json["final_diagnosis_name"] = DIAGNOSIS_ID_MAP[top_candidates[0]]
                    parsed_json["status"] = "Emergency_Fallback"
                elif "01" in DIAGNOSIS_ID_MAP:
                    parsed_json["final_diagnosis_id"] = "01"
                    parsed_json["final_diagnosis_name"] = DIAGNOSIS_ID_MAP["01"]
                    parsed_json["status"] = "Emergency_Fallback"
        else:
            # ID æœ‰æ•ˆï¼Œç¡®ä¿åç§°æ­£ç¡®
            parsed_json["final_diagnosis_name"] = DIAGNOSIS_ID_MAP.get(
                final_diagnosis_id,
                parsed_json.get("final_diagnosis_name", "Unknown")
            )
            
            # ç¡®å®šçŠ¶æ€ (å¼ºåˆ¶è¦†ç›– LLM è¾“å‡ºï¼Œç¡®ä¿é€»è¾‘ä¸€è‡´æ€§)
            # Confirm: Phase 1 çš„ final_diagnosis_id == Phase 3 çš„ final_diagnosis_id
            # Overturn: Phase 1 çš„ final_diagnosis_id != Phase 3 çš„ final_diagnosis_id
            phase1_final_id = phase1_result.get("final_diagnosis_id", "")
            if final_diagnosis_id == phase1_final_id:
                parsed_json["status"] = "Confirm"
            else:
                parsed_json["status"] = "Overturn"
        
        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        if "reasoning_path" not in parsed_json:
            parsed_json["reasoning_path"] = "Diagnosis selected based on graph analysis."
        if "audit_log" not in parsed_json:
            parsed_json["audit_log"] = []
        
        return parsed_json
