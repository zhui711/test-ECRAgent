#!/usr/bin/env python3
"""
Online Inference Script
========================

ä½¿ç”¨ Golden Graph + Memory Bank è¿›è¡Œ Online æ¨ç†çš„ä¸»å…¥å£è„šæœ¬ã€‚

åŠŸèƒ½ï¼š
1. åŠ è½½ test_verify/ ç›®å½•ä¸‹çš„æµ‹è¯•æ•°æ®
2. ä½¿ç”¨ Phase 2 Hybrid Engine (Golden Graph + Live Search)
3. ä½¿ç”¨ Memory-Augmented Judge (Few-Shot Context)
4. ä¿å­˜æ¨ç†ç»“æœå’Œå›¾è°±

Usage:
    python scripts/run_online_inference.py
    python scripts/run_online_inference.py --input-dir test_verify --output-dir output_online
    python scripts/run_online_inference.py --limit 10 --parallel --workers 4
    python scripts/run_online_inference.py --retry-failed
"""

import argparse
import json
import os
import sys
import time
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

import yaml
from dotenv import load_dotenv
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.utils.api_client import LLMClient
from src.utils.prompt_utils import DIAGNOSIS_ID_MAP
from src.graph.schema import MedicalGraph
from src.graph.state import AgentState
from src.graph.golden_graph_loader import GoldenGraphLoader
from src.agents.phase1_manager import Phase1Manager
from src.agents.phase2_hybrid_engine import Phase2HybridEngine
from src.agents.phase3_memory_judge import MemoryAugmentedJudge, create_judge_agent
from src.memory.memory_bank import MemoryBankManager
from src.utils.graph_tools import summarize_graph_for_judge, calculate_naive_scores


# ==================== æ•°æ®ç»“æ„ ====================

@dataclass
class OnlineResult:
    """å•ä¸ª Case çš„æ¨ç†ç»“æœ"""
    case_id: str
    case_type: str
    status: str  # "success" | "error"
    ground_truth_id: Optional[str] = None
    ground_truth_name: Optional[str] = None
    phase1_diagnosis_id: Optional[str] = None
    phase1_diagnosis_name: Optional[str] = None
    final_diagnosis_id: Optional[str] = None
    final_diagnosis_name: Optional[str] = None
    verdict_status: Optional[str] = None  # "Confirm" | "Overturn" | "Fallback"
    is_correct: bool = False
    processing_time: float = 0.0
    error_message: Optional[str] = None
    hybrid_stats: Optional[Dict[str, int]] = None
    memory_used: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


# ==================== è¾“å‡ºç®¡ç†å™¨ ====================

class OnlineOutputManager:
    """è¾“å‡ºç›®å½•å’Œæ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, output_dir: str = "output_online"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.graphs_dir = self.output_dir / "graphs"
        self.graphs_dir.mkdir(exist_ok=True)
        
        # ç»“æœæ–‡ä»¶
        self.results_file = self.output_dir / "results_detail.jsonl"
        self.summary_file = self.output_dir / "inference_summary.json"
        self.error_log_file = self.output_dir / "error_log.jsonl"
        
        print(f"[OutputManager] Output directory: {self.output_dir}")
    
    def save_result(self, result: OnlineResult) -> None:
        """è¿½åŠ ä¿å­˜å•ä¸ªç»“æœåˆ° JSONL"""
        with open(self.results_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(result.to_dict(), ensure_ascii=False) + "\n")
    
    def save_graph(self, case_id: str, graph_json: Dict[str, Any]) -> None:
        """ä¿å­˜å›¾è°± JSON"""
        filepath = self.graphs_dir / f"{case_id}_online_graph.json"
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(graph_json, f, indent=2, ensure_ascii=False)
    
    def save_error(self, case_id: str, case_type: str, error_message: str) -> None:
        """ä¿å­˜é”™è¯¯æ—¥å¿—"""
        error_record = {
            "case_id": case_id,
            "case_type": case_type,
            "error_message": error_message,
            "timestamp": datetime.now().isoformat()
        }
        with open(self.error_log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(error_record, ensure_ascii=False) + "\n")
    
    def save_summary(self, summary: Dict[str, Any]) -> None:
        """ä¿å­˜è¿è¡Œæ‘˜è¦"""
        with open(self.summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
    
    def load_processed_cases(self) -> Dict[str, str]:
        """åŠ è½½å·²å¤„ç†çš„ Caseï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰"""
        processed = {}
        if self.results_file.exists():
            with open(self.results_file, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        try:
                            record = json.loads(line)
                            key = f"{record['case_id']}_{record['case_type']}"
                            processed[key] = record.get("status", "unknown")
                        except:
                            pass
        return processed
    
    def get_error_cases(self) -> List[Tuple[str, str]]:
        """è·å–éœ€è¦é‡è¯•çš„é”™è¯¯ Case"""
        error_cases = []
        processed = self.load_processed_cases()
        
        for key, status in processed.items():
            if status == "error":
                parts = key.rsplit("_", 1)
                if len(parts) == 2:
                    case_id, case_type = parts
                    error_cases.append((case_id, case_type))
        
        return error_cases


# ==================== åœ¨çº¿æ¨ç†å™¨ ====================

class OnlineInferenceRunner:
    """Online æ¨ç†è¿è¡Œå™¨"""
    
    # æ¶ˆèå®éªŒ: ç”¨äºç¦ç”¨ Golden Graph çš„å‡ç›®å½•
    ABLATION_FAKE_DIR = "/tmp/__ablation_no_golden_graph__"
    
    def __init__(
        self,
        config: Dict[str, Any],
        output_manager: OnlineOutputManager,
        golden_graph_dir: str = "golden_graphs",
        memory_bank_dir: str = "memory_bank",
        use_memory: bool = True,
        use_golden_graph: bool = True,  # [æ¶ˆèå®éªŒ] æ˜¯å¦ä½¿ç”¨ Golden Graph
        parallel: bool = False,
        max_workers: int = 4
    ):
        """
        åˆå§‹åŒ– Online æ¨ç†è¿è¡Œå™¨
        
        Args:
            config: é…ç½®å­—å…¸
            output_manager: è¾“å‡ºç®¡ç†å™¨
            golden_graph_dir: Golden Graph ç›®å½•
            memory_bank_dir: Memory Bank ç›®å½•
            use_memory: æ˜¯å¦ä½¿ç”¨ Memory Bank
            use_golden_graph: [æ¶ˆèå®éªŒ] æ˜¯å¦ä½¿ç”¨ Golden Graph (False æ—¶é™çº§ä¸ºçº¯ Live Search)
            parallel: æ˜¯å¦å¹¶å‘å¤„ç†
            max_workers: å¹¶å‘ worker æ•°é‡
        """
        self.config = config
        self.output_manager = output_manager
        self.parallel = parallel
        self.max_workers = max_workers
        self.use_memory = use_memory
        self.use_golden_graph = use_golden_graph  # è®°å½•æ¶ˆèçŠ¶æ€
        
        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        api_config = config.get("api", {})
        api_key = os.getenv("YUNWU_API_KEY")
        if not api_key:
            raise ValueError("YUNWU_API_KEY not found in environment")
        
        self.llm_client = LLMClient(
            base_url=api_config.get("base_url", "https://yunwu.ai/v1"),
            api_key=api_key,
            timeout=api_config.get("timeout", 120)
        )
        self.model_name = api_config.get("model_name", "qwen3-32b")
        
        # åˆå§‹åŒ– Phase 1
        self.phase1 = Phase1Manager(self.llm_client, self.model_name)
        
        # [æ¶ˆèå®éªŒ] åˆå§‹åŒ– Phase 2 Hybrid Engine
        # å½“ use_golden_graph=False æ—¶ï¼Œä¼ å…¥å‡ç›®å½•ç¦ç”¨ Golden Graph
        if use_golden_graph:
            actual_gg_dir = golden_graph_dir
            actual_refined_dir = "golden_graphs_refined"  # é»˜è®¤ç²¾ç‚¼ç›®å½•
        else:
            # æ¶ˆèæ¨¡å¼: ä¼ å…¥ä¸å­˜åœ¨çš„å‡ç›®å½•ï¼Œè®© GoldenGraphLoader åŠ è½½ 0 ä¸ªå›¾
            actual_gg_dir = self.ABLATION_FAKE_DIR
            actual_refined_dir = self.ABLATION_FAKE_DIR
            print(f"[OnlineRunner] âš ï¸ ABLATION MODE: Golden Graph disabled (using fake dir)")
        
        self.phase2 = Phase2HybridEngine(
            llm_client=self.llm_client,
            model_name=self.model_name,
            golden_graph_dir=actual_gg_dir,
            refined_graph_dir=actual_refined_dir  # ä¼ å…¥æ¶ˆèç›®å½•
        )
        
        # åˆå§‹åŒ– Memory Bank
        if use_memory:
            self.memory_bank = MemoryBankManager(output_dir=memory_bank_dir)
            try:
                self.memory_bank.load()
                print(f"[OnlineRunner] Memory Bank loaded: {self.memory_bank.get_statistics()}")
            except Exception as e:
                print(f"[OnlineRunner] Warning: Failed to load Memory Bank: {e}")
                self.memory_bank = None
        else:
            self.memory_bank = None
        
        # åˆå§‹åŒ– Phase 3 Memory-Augmented Judge
        self.phase3 = create_judge_agent(
            llm_client=self.llm_client,
            model_name=self.model_name,
            # use_memory=False,
            # memory_bank=None
            use_memory=use_memory and self.memory_bank is not None,
            memory_bank=self.memory_bank
        )
        
        print(f"[OnlineRunner] Initialized with:")
        print(f"  - Model: {self.model_name}")
        print(f"  - Golden Graph: {'DISABLED (Ablation)' if not use_golden_graph else golden_graph_dir}")
        print(f"  - Memory Bank: {'Enabled' if use_memory and self.memory_bank else 'Disabled'}")
        print(f"  - Parallel: {parallel} (workers: {max_workers})")
    
    def process_single_case(
        self,
        case_dir: Path,
        case_type: str
    ) -> OnlineResult:
        """
        å¤„ç†å•ä¸ª Case
        
        Args:
            case_dir: Case ç›®å½•
            case_type: "control_case" æˆ– "trap_case"
        
        Returns:
            OnlineResult å®ä¾‹
        """
        case_id = case_dir.name
        start_time = time.time()
        
        try:
            # åŠ è½½ Case æ•°æ® (ä¸ io_utils.load_case_data ä¿æŒä¸€è‡´)
            case_file = case_dir / "final_benchmark_pair.json"
            if not case_file.exists():
                raise FileNotFoundError(f"final_benchmark_pair.json not found in {case_dir}")
            
            with open(case_file, "r", encoding="utf-8") as f:
                case_data = json.load(f)
            
            # è·å–å¯¹åº”ç±»å‹çš„æ•°æ® (key æ˜¯ "control_case" æˆ– "trap_case")
            if case_type not in case_data:
                raise ValueError(f"Case type '{case_type}' not found in final_benchmark_pair.json")
            
            case_info = case_data[case_type]
            narrative = case_info.get("narrative", "")
            ground_truth = case_info.get("ground_truth", "")
            
            # è§£æ Ground Truth ID
            gt_id = None
            for did, dname in DIAGNOSIS_ID_MAP.items():
                if dname.lower() == ground_truth.lower():
                    gt_id = did
                    break
            
            # Phase 1
            phase1_result = self.phase1.process(narrative)
            
            if phase1_result.get("error"):
                raise Exception(f"Phase 1 error: {phase1_result['error']}")
            
            # æ„å»ºåˆå§‹çŠ¶æ€ï¼ˆåŒ…å«æ‰€æœ‰ AgentState å­—æ®µï¼‰
            state: AgentState = {
                "case_id": case_id,
                "input_case": {
                    "narrative": narrative,
                    "ground_truth": ground_truth,
                    "ground_truth_id": gt_id
                },
                "phase1_result": phase1_result,
                "graph_json": None,
                "graph_summary": None,
                "naive_scores": None,
                "final_output": None,
                "status": "processing",
                "error_log": None,
                "global_hint": None,
                "retry_count": 0,
                "memory_context": None,
                "memory_records": None  # Memory Bank æ£€ç´¢ç»“æœ
            }
            
            # Phase 2 (Hybrid Engine)
            state = self.phase2.process(state)
            
            if state.get("status") == "failed":
                raise Exception(f"Phase 2 error: {state.get('error_log')}")
            
            # ç”Ÿæˆ Graph Summary å’Œ Naive Scores
            graph_json = state.get("graph_json", {})
            
            graph_summary = summarize_graph_for_judge(graph_json)
            state["graph_summary"] = graph_summary
            
            naive_scores = calculate_naive_scores(graph_json)
            state["naive_scores"] = naive_scores
            
            # Phase 3 (Memory-Augmented Judge)
            state = self.phase3.process(state)
            
            if state.get("status") == "failed":
                raise Exception(f"Phase 3 error: {state.get('error_log')}")
            
            # æå–ç»“æœ
            final_output = state.get("final_output", {})
            final_id = final_output.get("final_diagnosis_id")
            final_name = final_output.get("final_diagnosis_name")
            verdict_status = final_output.get("status")
            
            phase1_id = phase1_result.get("final_diagnosis_id")
            phase1_name = DIAGNOSIS_ID_MAP.get(phase1_id, "Unknown")
            
            is_correct = (final_id == gt_id) if gt_id else False
            
            # ä¿å­˜å›¾è°±
            self.output_manager.save_graph(f"{case_id}_{case_type}", graph_json)
            
            processing_time = time.time() - start_time
            
            # è·å– Hybrid Engine ç»Ÿè®¡ï¼ˆä» graph_json è·å–ï¼Œé¿å…å¹¶å‘æ—¶çš„ç«æ€æ¡ä»¶ï¼‰
            hybrid_stats = graph_json.get("hybrid_engine_stats", {})
            memory_used = final_output.get("memory_retrieval_used", False)
            
            return OnlineResult(
                case_id=case_id,
                case_type=case_type,
                status="success",
                ground_truth_id=gt_id,
                ground_truth_name=ground_truth,
                phase1_diagnosis_id=phase1_id,
                phase1_diagnosis_name=phase1_name,
                final_diagnosis_id=final_id,
                final_diagnosis_name=final_name,
                verdict_status=verdict_status,
                is_correct=is_correct,
                processing_time=processing_time,
                error_message=None,
                hybrid_stats=hybrid_stats,
                memory_used=memory_used
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"{str(e)}\n{traceback.format_exc()}"
            
            # ä¿å­˜é”™è¯¯æ—¥å¿—
            self.output_manager.save_error(case_id, case_type, error_msg)
            
            return OnlineResult(
                case_id=case_id,
                case_type=case_type,
                status="error",
                processing_time=processing_time,
                error_message=str(e)
            )
    
    def run(
        self,
        case_dirs: List[Path],
        case_types: List[str],
        retry_failed: bool = False
    ) -> List[OnlineResult]:
        """
        è¿è¡Œæ¨ç†
        
        Args:
            case_dirs: Case ç›®å½•åˆ—è¡¨
            case_types: è¦å¤„ç†çš„ Case ç±»å‹åˆ—è¡¨
            retry_failed: æ˜¯å¦åªé‡è¯•ä¹‹å‰å¤±è´¥çš„ Case
        
        Returns:
            OnlineResult åˆ—è¡¨
        """
        # æ„å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        
        if retry_failed:
            # åªå¤„ç†ä¹‹å‰å¤±è´¥çš„ Case
            error_cases = self.output_manager.get_error_cases()
            for case_id, case_type in error_cases:
                for case_dir in case_dirs:
                    if case_dir.name == case_id:
                        tasks.append((case_dir, case_type))
                        break
            print(f"[OnlineRunner] Retrying {len(tasks)} failed cases")
        else:
            # å¤„ç†æ‰€æœ‰ Case
            processed = self.output_manager.load_processed_cases()
            
            for case_dir in case_dirs:
                for case_type in case_types:
                    key = f"{case_dir.name}_{case_type}"
                    if key not in processed or processed[key] == "error":
                        tasks.append((case_dir, case_type))
            
            print(f"[OnlineRunner] Processing {len(tasks)} cases")
        
        if not tasks:
            print("[OnlineRunner] No cases to process")
            return []
        
        # æ‰§è¡Œæ¨ç†
        if self.parallel:
            results = self._run_parallel(tasks)
        else:
            results = self._run_sequential(tasks)
        
        return results
    
    def _run_sequential(self, tasks: List[Tuple[Path, str]]) -> List[OnlineResult]:
        """ä¸²è¡Œæ‰§è¡Œ"""
        results = []
        
        with tqdm(total=len(tasks), desc="Online Inference") as pbar:
            for case_dir, case_type in tasks:
                result = self.process_single_case(case_dir, case_type)
                results.append(result)
                
                # ä¿å­˜ç»“æœ
                self.output_manager.save_result(result)
                
                # æ›´æ–°è¿›åº¦æ¡
                status_icon = "âœ“" if result.status == "success" else "âœ—"
                correct_icon = "ğŸ¯" if result.is_correct else ""
                pbar.set_postfix_str(f"{status_icon} {result.case_id} {correct_icon}")
                pbar.update(1)
        
        return results
    
    def _run_parallel(self, tasks: List[Tuple[Path, str]]) -> List[OnlineResult]:
        """å¹¶å‘æ‰§è¡Œ"""
        results = []
        
        with tqdm(total=len(tasks), desc="Online Inference (Parallel)") as pbar:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_task = {
                    executor.submit(self.process_single_case, case_dir, case_type): (case_dir, case_type)
                    for case_dir, case_type in tasks
                }
                
                for future in as_completed(future_to_task):
                    try:
                        result = future.result()
                    except Exception as e:
                        case_dir, case_type = future_to_task[future]
                        result = OnlineResult(
                            case_id=case_dir.name,
                            case_type=case_type,
                            status="error",
                            error_message=str(e)
                        )
                    
                    results.append(result)
                    self.output_manager.save_result(result)
                    
                    status_icon = "âœ“" if result.status == "success" else "âœ—"
                    pbar.set_postfix_str(f"{status_icon} {result.case_id}")
                    pbar.update(1)
        
        return results


# ==================== é…ç½®å’Œå‚æ•° ====================

def load_config() -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = PROJECT_ROOT / "config" / "settings.yaml"
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="MDT-Agent Online Inference with Golden Graph + Memory Bank",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # è¿è¡Œæ‰€æœ‰æµ‹è¯• Case
  python scripts/run_online_inference.py
  
  # é™åˆ¶å¤„ç†æ•°é‡
  python scripts/run_online_inference.py --limit 10
  
  # å¹¶å‘å¤„ç†
  python scripts/run_online_inference.py --parallel --workers 4
  
  # é‡è¯•å¤±è´¥çš„ Case
  python scripts/run_online_inference.py --retry-failed
  
  # ä¸ä½¿ç”¨ Memory Bank
  python scripts/run_online_inference.py --no-memory
        """
    )
    
    parser.add_argument(
        "--input-dir",
        type=str,
        default="test_verify",
        help="è¾“å…¥æ•°æ®ç›®å½•ï¼ˆé»˜è®¤: test_verifyï¼‰"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default="output_online",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: output_onlineï¼‰"
    )
    
    parser.add_argument(
        "--golden-graph-dir",
        type=str,
        default="golden_graphs_refined",
        help="Golden Graph ç›®å½•ï¼ˆé»˜è®¤: golden_graphs_refinedï¼‰"
    )
    
    parser.add_argument(
        "--memory-bank-dir",
        type=str,
        default="memory_bank",
        help="Memory Bank ç›®å½•ï¼ˆé»˜è®¤: memory_bankï¼‰"
    )
    
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="é™åˆ¶å¤„ç†çš„ Case æ•°é‡"
    )
    
    parser.add_argument(
        "--case-ids",
        type=str,
        default=None,
        help="æŒ‡å®šè¦å¤„ç†çš„ Case IDï¼ˆé€—å·åˆ†éš”ï¼‰"
    )
    
    parser.add_argument(
        "--skip-control",
        action="store_true",
        help="è·³è¿‡ control_case"
    )
    
    parser.add_argument(
        "--skip-trap",
        action="store_true",
        help="è·³è¿‡ trap_case"
    )
    
    parser.add_argument(
        "--parallel",
        action="store_true",
        help="å¯ç”¨å¹¶å‘å¤„ç†"
    )
    
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="å¹¶å‘ worker æ•°é‡ï¼ˆé»˜è®¤: 4ï¼‰"
    )
    
    parser.add_argument(
        "--no-memory",
        action="store_true",
        help="ä¸ä½¿ç”¨ Memory Bank"
    )
    
    parser.add_argument(
        "--no-golden-graph",
        action="store_true",
        help="[æ¶ˆèå®éªŒ] ä¸ä½¿ç”¨ Golden Graphï¼Œä»…ä½¿ç”¨ Live Search"
    )
    
    parser.add_argument(
        "--retry-failed",
        action="store_true",
        help="åªé‡è¯•ä¹‹å‰å¤±è´¥çš„ Case"
    )
    
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹åç§°"
    )
    
    return parser.parse_args()


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è§£æå‚æ•°
    args = parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # è¦†ç›–æ¨¡å‹åç§°
    if args.model:
        config["api"]["model_name"] = args.model
    
    # åˆå§‹åŒ–è¾“å‡ºç®¡ç†å™¨
    output_manager = OnlineOutputManager(args.output_dir)
    
    # è·å– Case ç›®å½•åˆ—è¡¨
    input_path = PROJECT_ROOT / args.input_dir
    if not input_path.exists():
        print(f"âŒ Input directory not found: {input_path}")
        sys.exit(1)
    
    # ç­›é€‰ Case ç›®å½•
    if args.case_ids:
        specified_ids = set(args.case_ids.split(","))
        case_dirs = sorted([
            d for d in input_path.iterdir()
            if d.is_dir() and d.name in specified_ids
        ])
    else:
        case_dirs = sorted([
            d for d in input_path.iterdir()
            if d.is_dir() and d.name.startswith("case_")
        ])
    
    # åº”ç”¨ limit
    if args.limit:
        case_dirs = case_dirs[:args.limit]
    
    # ç¡®å®šè¦å¤„ç†çš„ Case ç±»å‹
    case_types = []
    if not args.skip_control:
        case_types.append("control_case")
    if not args.skip_trap:
        case_types.append("trap_case")
    
    if not case_types:
        print("âŒ No case types to process")
        sys.exit(1)
    
    # [æ¶ˆèå®éªŒ] æ£€æµ‹æ¶ˆèæ¨¡å¼
    use_golden_graph = not getattr(args, 'no_golden_graph', False)
    ablation_mode = args.no_memory or not use_golden_graph
    
    # æ‰“å°è¿è¡Œä¿¡æ¯
    print("=" * 70)
    if ablation_mode:
        print("MDT-Agent Online Inference [ABLATION MODE]")
    else:
        print("MDT-Agent Online Inference (Golden Graph + Memory Bank)")
    print("=" * 70)
    print(f"ğŸ“ Input directory: {input_path}")
    print(f"ğŸ“ Output directory: {output_manager.output_dir}")
    print(f"ğŸ“ Golden Graph: {'âš ï¸ DISABLED (Ablation)' if not use_golden_graph else args.golden_graph_dir}")
    print(f"ğŸ“ Memory Bank: {args.memory_bank_dir}")
    print(f"ğŸ“Š Total case directories: {len(case_dirs)}")
    print(f"ğŸ“‹ Case types: {', '.join(case_types)}")
    print(f"ğŸ”§ Model: {config['api']['model_name']}")
    print(f"ğŸ“Š Golden Graph: {'âš ï¸ DISABLED (Ablation)' if not use_golden_graph else 'Enabled'}")
    print(f"ğŸ§  Memory Bank: {'âš ï¸ DISABLED (Ablation)' if args.no_memory else 'Enabled'}")
    print(f"âš¡ Parallel mode: {'Yes' if args.parallel else 'No'}")
    if args.parallel:
        print(f"ğŸ‘· Workers: {args.workers}")
    if ablation_mode:
        print("-" * 70)
        print("âš ï¸  ABLATION EXPERIMENT: Some offline components are disabled")
    print("=" * 70)
    
    # åˆå§‹åŒ–è¿è¡Œå™¨
    try:
        runner = OnlineInferenceRunner(
            config=config,
            output_manager=output_manager,
            golden_graph_dir=args.golden_graph_dir,
            memory_bank_dir=args.memory_bank_dir,
            use_memory=not args.no_memory,
            use_golden_graph=use_golden_graph,  # [æ¶ˆèå®éªŒ] æ§åˆ¶ Golden Graph
            parallel=args.parallel,
            max_workers=args.workers
        )
    except Exception as e:
        print(f"âŒ Failed to initialize runner: {e}")
        traceback.print_exc()
        sys.exit(1)
    
    # è¿è¡Œæ¨ç†
    start_time = time.time()
    results = runner.run(
        case_dirs=case_dirs,
        case_types=case_types,
        retry_failed=args.retry_failed
    )
    total_time = time.time() - start_time
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results if r.status == "success")
    error_count = sum(1 for r in results if r.status == "error")
    correct_count = sum(1 for r in results if r.is_correct)
    
    # æŒ‰ Case Type ç»Ÿè®¡
    control_results = [r for r in results if r.case_type == "control_case"]
    trap_results = [r for r in results if r.case_type == "trap_case"]
    
    control_correct = sum(1 for r in control_results if r.is_correct)
    trap_correct = sum(1 for r in trap_results if r.is_correct)
    
    # ç»Ÿè®¡ Overturn
    overturn_count = sum(1 for r in results if r.verdict_status == "Overturn")
    
    # ä¿å­˜æ‘˜è¦
    summary = {
        "run_timestamp": datetime.now().isoformat(),
        "total_processed": len(results),
        "success": success_count,
        "error": error_count,
        "correct": correct_count,
        "accuracy": correct_count / success_count if success_count > 0 else 0,
        "control_accuracy": control_correct / len(control_results) if control_results else 0,
        "trap_accuracy": trap_correct / len(trap_results) if trap_results else 0,
        "overturn_count": overturn_count,
        "total_time_seconds": total_time,
        "avg_time_per_case": total_time / len(results) if results else 0,
        "config": {
            "model": config["api"]["model_name"],
            "use_memory": not args.no_memory,
            "golden_graph_dir": args.golden_graph_dir,
            "memory_bank_dir": args.memory_bank_dir,
            "parallel": args.parallel,
            "workers": args.workers if args.parallel else 1
        }
    }
    output_manager.save_summary(summary)
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 70)
    print("Online Inference Complete!")
    print("=" * 70)
    print(f"âœ… Success: {success_count}")
    print(f"âŒ Error: {error_count}")
    print(f"ğŸ¯ Correct: {correct_count}/{success_count} ({summary['accuracy']:.2%})")
    if control_results:
        print(f"  - Control: {control_correct}/{len(control_results)} ({summary['control_accuracy']:.2%})")
    if trap_results:
        print(f"  - Trap: {trap_correct}/{len(trap_results)} ({summary['trap_accuracy']:.2%})")
    print(f"ğŸ”„ Overturn: {overturn_count}")
    print(f"â±ï¸  Total time: {total_time:.1f}s")
    print(f"ğŸ“Š Results saved to: {output_manager.output_dir}")
    print("=" * 70)


if __name__ == "__main__":
    main()

